{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 2\n",
        "\n",
        "âš ï¸ **Important! Submission Requirements**\n",
        "\n",
        "1. **Do NOT clear notebook logs or outputs.**  \n",
        "   Every output cell must remain visible (e.g., print logs, progress logs, summary messages).\n",
        "\n",
        "2. **The notebook must be *self-contained*.**  \n",
        "   - You should be able to run it **from top to bottom once** without any manual steps.  \n",
        "   - No need for any external scripts, config files, or multiple runs.\n",
        "   - Creating multiple cells/blocks is OK.\n",
        "\n",
        "3. **Single `.ipynb` implementation.**  \n",
        "   Do not create separate Python files or folders other than the generated image output directory.\n",
        "\n",
        "4. **Clean, readable, and well-commented code.**\n",
        "\n",
        "5. **Submission file name format:**  \n",
        "   â†’ `studentID_name.zip`  \n",
        "   (Example: `2025000000_jungbeomlee.zip`)\n",
        "   - This file should contain `COSE-474-02-Assignment_2.ipynb`.\n",
        "---\n"
      ],
      "metadata": {
        "id": "HbcdIbJrUwOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Build Your Own RNN (Forward Only)\n",
        "**ğŸ¯ Goal**\n",
        "\n",
        "\n",
        "In this problem, you will implement a simple (vanilla) RNN from scratch in PyTorch and verify that its forward pass matches torch.nn.RNN when they share the same parameters.\n",
        "\n",
        "You will not train this RNN. The goal is to:\n",
        "\n",
        "- Understand how the hidden state is updated over time.\n",
        "\n",
        "- Implement the forward computation manually.\n",
        "\n",
        "- Run a small sanity check to confirm that your implementation is correct.\n",
        "\n",
        "We will implement:\n",
        "\n",
        "1. MyRNNCell: computes a single RNN step $h_t$ from $(x_t, h_{t-1})$.\n",
        "\n",
        "2. MyRNN: applies MyRNNCell repeatedly over the time dimension.\n",
        "\n",
        "3. A sanity check: copy weights from nn.RNN to MyRNN and verify that the outputs match.\n",
        "\n",
        "Please fill in all TODO parts. Do not change function signatures."
      ],
      "metadata": {
        "id": "AmdrHdWBVZIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "u0b92waAUyMW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7c3d83b-2fe6-4368-bd31-19684354eb72"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyRNNCell(nn.Module):\n",
        "    \"\"\"\n",
        "    A single vanilla RNN cell:\n",
        "        h_t = tanh(Wx * x_t + Wh * h_{t-1} + b_h)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        # define your weights, ignore output head (Wyh in our lecture note) for this assignment\n",
        "        self.Wx = nn.Linear(input_dim, hidden_dim, bias=False) # bias: False\n",
        "        self.Wh = nn.Linear(hidden_dim, hidden_dim, bias=True) # bias: True\n",
        "\n",
        "    def forward(self, x_t: torch.Tensor, h_prev: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_t:   (batch_size, input_dim)\n",
        "            h_prev:(batch_size, hidden_dim)\n",
        "        Returns:\n",
        "            h_t:   (batch_size, hidden_dim)\n",
        "        \"\"\"\n",
        "\n",
        "        # implement your codes to compute h_t from x_t and h_prev\n",
        "        input_part = self.Wx(x_t)\n",
        "        hidden_part = self.Wh(h_prev)\n",
        "\n",
        "        # tanh í™œì„±í™” í•¨ìˆ˜ ì ìš©\n",
        "        h_t = torch.tanh(input_part + hidden_part)\n",
        "\n",
        "        return h_t"
      ],
      "metadata": {
        "id": "Mf4yyh_0W3Fb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple RNN that applies MyRNNCell over a sequence.\n",
        "    We assume batch_first=True, i.e., input shape is (batch, seq_len, input_dim).\n",
        "    Initialize h_0 as a zero tensor\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.cell = MyRNNCell(input_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, h0: torch.Tensor | None = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x:  (batch_size, seq_len, input_dim)\n",
        "            h0: (batch_size, hidden_dim) initial hidden state, or None\n",
        "        Returns:\n",
        "            outputs: (batch_size, seq_len, hidden_dim)\n",
        "            h_T:     (batch_size, hidden_dim)  # hidden state at last time step\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # ì€ë‹‰ ìƒíƒœ ì´ˆê¸°í™”\n",
        "        if h0 is None:\n",
        "            h0 = torch.zeros(batch_size, self.hidden_dim, device=x.device)\n",
        "\n",
        "        h_t = h0\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        # TODO: iterate over time steps and apply the RNN cell\n",
        "\n",
        "        # seq_len loop êµ¬ì„±\n",
        "        for t in range(seq_len):\n",
        "            x_t = x[:, t, :]\n",
        "            h_t = self.cell(x_t, h_t)\n",
        "            outputs.append(h_t)\n",
        "\n",
        "        # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ëœ í…ì„œë“¤ì„ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ìŠ¤íƒœí‚¹\n",
        "        outputs = torch.stack(outputs, dim=1)\n",
        "\n",
        "        # ë§ˆì§€ë§‰ ì‹œì ì˜ ì€ë‹‰ ìƒíƒœ\n",
        "        h_T = h_t\n",
        "\n",
        "        return outputs, h_T"
      ],
      "metadata": {
        "id": "JXs9axd1XaLP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity Check: Compare with torch.nn.RNN\n",
        "\n",
        "Here we:\n",
        "\n",
        "- Create a PyTorch nn.RNN and your MyRNN with the same dimensions.\n",
        "\n",
        "- Copy the parameters from nn.RNN into MyRNN so they represent the same RNN.\n",
        "\n",
        "- Feed the same input and compare outputs.\n",
        "\n",
        "If your implementation is correct, the mean absolute difference should be very small (e.g., < 1e-6)."
      ],
      "metadata": {
        "id": "7HReVqn6ZfIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters for the sanity check\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "input_dim = 4\n",
        "hidden_dim = 3\n",
        "\n",
        "# Create PyTorch RNN and your MyRNN\n",
        "torch_rnn = nn.RNN(input_dim, hidden_dim, batch_first=True, nonlinearity=\"tanh\")\n",
        "my_rnn    = MyRNN(input_dim, hidden_dim)\n",
        "\n",
        "# Move to device (optional, but good practice)\n",
        "torch_rnn = torch_rnn.to(device)\n",
        "my_rnn    = my_rnn.to(device)\n",
        "\n",
        "# 1) Copy weights from torch_rnn to my_rnn\n",
        "with torch.no_grad():\n",
        "    my_rnn.cell.Wx.weight.copy_(torch_rnn.weight_ih_l0)\n",
        "\n",
        "    # Copy hidden-to-hidden weights\n",
        "    my_rnn.cell.Wh.weight.copy_(torch_rnn.weight_hh_l0)\n",
        "\n",
        "    # Combine the two biases from torch_rnn into one bias in MyRNNCell\n",
        "    combined_bias = torch_rnn.bias_ih_l0 + torch_rnn.bias_hh_l0\n",
        "    my_rnn.cell.Wh.bias.copy_(combined_bias)\n",
        "\n",
        "# 2) Create a random input and initial hidden state\n",
        "x = torch.randn(batch_size, seq_len, input_dim, device=device)\n",
        "\n",
        "# torch.nn.RNN expects h0 of shape (num_layers, batch, hidden_dim)\n",
        "h0_torch = torch.randn(1, batch_size, hidden_dim, device=device)\n",
        "h0_my    = h0_torch.squeeze(0)  # (batch, hidden_dim)\n",
        "\n",
        "# 3) Forward pass\n",
        "out_torch, h_torch = torch_rnn(x, h0_torch)      # out_torch: (B, L, H), h_torch: (1, B, H)\n",
        "out_my, h_my       = my_rnn(x, h0_my)            # out_my:   (B, L, H), h_my:    (B, H)\n",
        "\n",
        "# 4) Compare outputs\n",
        "output_diff = torch.abs(out_torch - out_my).mean().item()\n",
        "hidden_diff = torch.abs(h_torch.squeeze(0) - h_my).mean().item()\n",
        "\n",
        "print(f\"Mean absolute difference (outputs): {output_diff:.8f}\")\n",
        "print(f\"Mean absolute difference (hidden):  {hidden_diff:.8f}\")\n",
        "\n",
        "assert output_diff < 1e-4 and hidden_diff < 1e-4, \"RNN implementation does not match torch.nn.RNN!\"\n",
        "print(\"âœ… Sanity check passed! Your MyRNN matches torch.nn.RNN (forward).\")"
      ],
      "metadata": {
        "id": "d70-rq7AYENE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a379f2-11eb-4dfc-880c-0a0e0a447150"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean absolute difference (outputs): 0.00000215\n",
            "Mean absolute difference (hidden):  0.00000174\n",
            "âœ… Sanity check passed! Your MyRNN matches torch.nn.RNN (forward).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Word-level Language Modeling with LSTM\n",
        "\n",
        "In this problem, you will implement a simple word-level language model using an LSTM in PyTorch.\n",
        "You will:\n",
        "\n",
        "1. Load a text corpus from corpus.txt (Tiny Shakespeare).\n",
        "\n",
        "2. Tokenize the corpus into words and build a vocabulary.\n",
        "\n",
        "3. Create a dataset of inputâ€“target word sequences for next-word prediction.\n",
        "\n",
        "4. Implement an LSTM-based language model using nn.Embedding, nn.LSTM, and nn.Linear.\n",
        "\n",
        "5. Train the model to minimize cross-entropy loss using teacher-forcing.\n",
        "\n",
        "6. Implement a function to generate text given a prompt.\n",
        "\n",
        "For an input sequence of length $L$,\n",
        "- Input: $[w_1, w_2, ..., w_L]$\n",
        "- Output: $[w_2, w_3, ..., w_{L+1}]$\n",
        "\n",
        "\n",
        "Requirements:\n",
        "1. Training loss should be under 0.5.\n",
        "2. Try multiple inputs to \"generate_text\" function and see if your model generates feasible texts."
      ],
      "metadata": {
        "id": "eNShDsRQahe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "Z4I-fRP2diGA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d088f73-3f40-4ed5-b586-ad9bd144d4bf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download 'corpus.txt' and upload it to the current directory.\n",
        "\n",
        "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Total characters:\", len(text))\n",
        "print(\"Sample text:\")\n",
        "print(text[:500])"
      ],
      "metadata": {
        "id": "ce5aXRmjd1eF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8921fd9b-f6a9-45b9-ee0d-a694b299c707"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 1115393\n",
            "Sample text:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple word-level tokenization: split on whitespace.\n",
        "# (You may modify this if you want, but keep it simple.)\n",
        "\n",
        "tokens = text.strip().split()\n",
        "print(\"Number of tokens:\", len(tokens))\n",
        "print(\"First 50 tokens:\", tokens[:50])"
      ],
      "metadata": {
        "id": "kmEEN1rUd-KW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e35b09-f8bb-4c52-f25c-a329af1b97a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 202651\n",
            "First 50 tokens: ['First', 'Citizen:', 'Before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.', 'All:', 'Speak,', 'speak.', 'First', 'Citizen:', 'You', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish?', 'All:', 'Resolved.', 'resolved.', 'First', 'Citizen:', 'First,', 'you', 'know', 'Caius', 'Marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people.', 'All:', 'We', \"know't,\", 'we', \"know't.\", 'First', 'Citizen:', 'Let', 'us']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary: map each unique word to an integer id.\n",
        "vocab = sorted(set(tokens))\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "metadata": {
        "id": "7n9BSIo-eFxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35798c9e-68bb-4690-dda1-56fd62a1d4a3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 25670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 20\n",
        "batch_size = 10 # you may change this\n",
        "\n",
        "class WordLMDataset(Dataset):\n",
        "    def __init__(self, tokens, seq_len, word_to_idx):\n",
        "        self.seq_len = seq_len\n",
        "        self.word_to_idx = word_to_idx\n",
        "\n",
        "        # Convert entire corpus into indices once.\n",
        "        self.indices = [self.word_to_idx[w] for w in tokens]\n",
        "\n",
        "    def __len__(self):\n",
        "        # number of possible sliding windows\n",
        "\n",
        "        return len(self.indices) - self.seq_len - 1\n",
        "        # ============================\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # ====== YOUR CODE HERE ======\n",
        "        x = self.indices[idx : idx + self.seq_len]\n",
        "        y = self.indices[idx + 1 : idx + self.seq_len + 1]\n",
        "        x = torch.tensor(x, dtype=torch.long)\n",
        "        y = torch.tensor(y, dtype=torch.long)\n",
        "        # ============================\n",
        "\n",
        "        return x, y\n",
        "\n",
        "dataset = WordLMDataset(tokens, seq_len, word_to_idx)\n",
        "print(len(dataset), dataset[0][0].shape, dataset[0][1].shape)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
      ],
      "metadata": {
        "id": "7vguXIN0eR7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9882ec13-661d-4ffc-d273-19f117c8b7cc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "202630 torch.Size([20]) torch.Size([20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM Language Model\n",
        "class WordLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x:      (batch, seq_len) int64 word indices\n",
        "            hidden: (h0, c0) or None\n",
        "        Returns:\n",
        "            logits: (batch, seq_len, vocab_size)\n",
        "            hidden: (h_T, c_T)\n",
        "        \"\"\"\n",
        "\n",
        "        # ====== YOUR CODE HERE ======\n",
        "        # Embedding ì •ì˜\n",
        "        emb = self.embedding(x)\n",
        "\n",
        "        # LSTM ì •ì˜\n",
        "        output, hidden = self.lstm(emb, hidden)\n",
        "\n",
        "        # FCì¸µ ì •ì˜\n",
        "        logits = self.fc(output)\n",
        "        # ============================\n",
        "\n",
        "        return logits, hidden"
      ],
      "metadata": {
        "id": "DOqpaGFJesma"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "\n",
        "embed_dim = 128\n",
        "hidden_dim = 256\n",
        "num_layers = 1\n",
        "num_epochs = 10   # you may increase/decrease : lossê°’ í–¥ìƒì„ ìœ„í•´ 5íšŒì—ì„œ 10íšŒë¡œ ì¡°ì •\n",
        "learning_rate = 1e-3\n",
        "\n",
        "model = WordLSTM(vocab_size, embed_dim, hidden_dim, num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)   # (B, L)\n",
        "        y = y.to(device)   # (B, L)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # TODO: forward, compute loss, backward, step\n",
        "        # Remember:\n",
        "        #   logits: (B, L, V)\n",
        "        #   CrossEntropyLoss expects: (B*L, V) and targets (B*L,)\n",
        "\n",
        "        # ====== YOUR CODE HERE ======\n",
        "        # Forward step\n",
        "        logits, _ = model(x)\n",
        "\n",
        "        # Compute Loss\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # ============================\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_one_epoch(model, loader, optimizer, criterion, device)\n",
        "    print(f\"Epoch {epoch:02d} | train loss: {train_loss:.4f}\")"
      ],
      "metadata": {
        "id": "zLhmrM4VfC_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7448f92e-176b-44b4-e226-bee7ada463f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train loss: 3.4412\n",
            "Epoch 02 | train loss: 1.1077\n",
            "Epoch 03 | train loss: 0.7314\n",
            "Epoch 04 | train loss: 0.6195\n",
            "Epoch 05 | train loss: 0.5665\n",
            "Epoch 06 | train loss: 0.5345\n",
            "Epoch 07 | train loss: 0.5130\n",
            "Epoch 08 | train loss: 0.4979\n",
            "Epoch 09 | train loss: 0.4860\n",
            "Epoch 10 | train loss: 0.4767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_text, max_new_tokens=50, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model:      trained WordLSTM\n",
        "        start_text: string, seed prompt (e.g., \"To be or not to be\")\n",
        "        max_new_tokens: how many words to generate\n",
        "        temperature: softmax temperature (>0). Higher = more random.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize start_text into words\n",
        "    words = start_text.strip().split()\n",
        "\n",
        "    # Map to indices (OOV words are mapped to 0 or some fallback)\n",
        "    # Here, we simply skip unknown words or map them to 0.\n",
        "    idxs = []\n",
        "    for w in words:\n",
        "        if w in word_to_idx:\n",
        "            idxs.append(word_to_idx[w])\n",
        "        else:\n",
        "            # unknown word â†’ you can choose to skip or map to 0\n",
        "            # here we map to 0:\n",
        "            idxs.append(0)\n",
        "\n",
        "    x = torch.tensor([idxs], dtype=torch.long, device=device)  # (1, L)\n",
        "    hidden = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, hidden = model(x, hidden)\n",
        "            # Focus on the last time step\n",
        "            last_logits = logits[:, -1, :] / temperature   # (1, V)\n",
        "            probs = F.softmax(last_logits, dim=-1)         # (1, V)\n",
        "            next_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            words.append(idx_to_word[next_idx])\n",
        "\n",
        "            # Now feed only the last predicted token as next input\n",
        "            x = torch.tensor([[next_idx]], dtype=torch.long, device=device)\n",
        "\n",
        "    return \" \".join(words)"
      ],
      "metadata": {
        "id": "iiAZj7HaylZB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_text=\"To be\", max_new_tokens=30, temperature=1.0))"
      ],
      "metadata": {
        "id": "A9p0GMrsytpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a67bbce2-6eaa-464e-9ff0-eae238d5682d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be full quit of those my banishers, Stand I before thee here. Then if I shall do no more. All: The gods assist you! AUFIDIUS: And keep your honours safe! First\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_text=\"For the first time,\", max_new_tokens=30, temperature=1.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c5tWsOgAkm2",
        "outputId": "f0539ed8-256b-461e-8dd2-2df53789fced"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the first time, work, there a man not worth the presence speaks grace? I'll not buy you of your first queen's ghost, it should law For thine love, and heart Had I hear,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_text=\"AI is\", max_new_tokens=30, temperature=1.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MOk5F_PAqhu",
        "outputId": "2dd659e5-71ff-4bf8-dc1c-807cd934b757"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI is held through the hollow she's fair and he's gone. Children: What stay had we but Clarence? and he's gone. DUCHESS OF YORK: My pretty cousins, you mistake me much; I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## text generationì„ ë‹¤ì–‘í•œ start textë¡œ ì§„í–‰í•´ë³¸ ê²°ê³¼, í•™ìŠµí–ˆë˜ corpusì— ìˆëŠ” ë¬¸ì¥ê³¼ ìœ ì‚¬í•œ ë‹¨ì–´ê°€ ìˆëŠ” ê²½ìœ  ë¹„êµì  ì •í™•í•˜ê²Œ ìƒì„±í•´ë‚¸ ë°˜ë©´ ë¹ˆë²ˆí•˜ê²Œ ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ” ë¬¸ì¥ í˜¹ì€ \"AI\"ì²˜ëŸ¼ í•™ìŠµ ê°€ëŠ¥í•œ í† í°ì´ ì•„ë‹Œ ê²½ìš°ì—ëŠ” ë¬¸ë²•ì ìœ¼ë¡œ, ì˜ë¯¸ë¡ ì ìœ¼ë¡œ í¬ê²Œ ë¶€ì¡±í•¨ì„ ë‚˜íƒ€ë‚´ì—ˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "m_q7plcPaQFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë˜í•œ ë¬¸ì¥ì˜ í›„ë°˜ë¶€ë¡œ ê°ˆìˆ˜ë¡ ì˜ë¯¸ë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” ë‹¨ì–´ì˜ ë‚˜ì—´ì´ ë¹ˆë²ˆí•˜ê²Œ ë‚˜íƒ€ë‚¨ì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "XeTCJSgVcISZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Character-level Language Modeling with LSTM\n",
        "\n",
        "In this problem, you will build **a character-level language model** using PyTorchâ€™s nn.LSTM.\n",
        "Unlike word-level language modeling, here we treat each character as a token.\n",
        "\n",
        "You will:\n",
        "\n",
        "- Build a character vocabulary from the corpus.\n",
        "\n",
        "- Implement a CharLMDataset that creates (input, target) sequences for next-character prediction.\n",
        "\n",
        "- Implement a CharLSTM model using nn.Embedding and nn.LSTM.\n",
        "\n",
        "- Train the model with teacher forcing: at each time step, the model receives the ground-truth previous characters as input.\n",
        "\n",
        "\n",
        "We will use the same corpus file as in Problem 2.\n",
        "\n",
        "\n",
        "Requirements:\n",
        "1. Training loss should be under 1.1.\n",
        "2. Try multiple inputs to \"generate_characters\" function and see if your model generates feasible texts."
      ],
      "metadata": {
        "id": "KkhJjuvGBIXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "QHB_UilqBhfD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "891a07b2-b39c-4eb7-cc5e-72b66d85ff81"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the full corpus as a single string\n",
        "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Total number of characters in corpus:\", len(text))\n",
        "print(\"Preview:\")\n",
        "print(text[:500])"
      ],
      "metadata": {
        "id": "0XYPS5LyBkwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49885879-c735-4793-bb23-5b9a934696ad"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters in corpus: 1115393\n",
            "Preview:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build character vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
        "vocab_size_char = len(chars)"
      ],
      "metadata": {
        "id": "Zp6bLXDMBnuA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Character-level dataset\n",
        "\n",
        "seq_len_char = 100\n",
        "\n",
        "class CharLMDataset(Dataset):\n",
        "    def __init__(self, text: str, seq_len: int):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        # Convert entire text into a list of indices\n",
        "        self.indices = [char_to_idx[c] for c in text]\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.indices) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # ====== YOUR CODE HERE ======\n",
        "        x = self.indices[idx : idx + self.seq_len]\n",
        "        y = self.indices[idx + 1 : idx + self.seq_len + 1]\n",
        "        x = torch.tensor(x, dtype=torch.long)\n",
        "        y = torch.tensor(y, dtype=torch.long)\n",
        "        # ============================\n",
        "\n",
        "        return x, y\n",
        "\n",
        "dataset_char = CharLMDataset(text, seq_len_char)\n",
        "batch_size = 64\n",
        "\n",
        "loader_char = DataLoader(dataset_char, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "print(\"Number of training examples:\", len(dataset_char))\n",
        "x_example, y_example = next(iter(loader_char))\n",
        "print(\"Example batch shapes:\", x_example.shape, y_example.shape)  # (B, L), (B, L)"
      ],
      "metadata": {
        "id": "6OEciq2EBu8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66dc3e74-a1d8-4a08-ccfb-193ab964e25a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 1115293\n",
            "Example batch shapes: torch.Size([64, 100]) torch.Size([64, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Character-level LSTM model\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embed_dim: int = 128, hidden_dim: int = 256, num_layers: int = 1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, hidden=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x:      (batch_size, seq_len)  integer character indices\n",
        "            hidden: (h_0, c_0) or None\n",
        "        Returns:\n",
        "            logits: (batch_size, seq_len, vocab_size)\n",
        "            hidden: (h_T, c_T)\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # ====== YOUR CODE HERE ======\n",
        "        # WordLSTMê³¼ ìœ ì‚¬\n",
        "        emb = self.embedding(x)\n",
        "        output, hidden = self.lstm(emb, hidden)\n",
        "        logits = self.fc(output)\n",
        "        # ============================\n",
        "\n",
        "        return logits, hidden"
      ],
      "metadata": {
        "id": "Jl5RmsT8CSN6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "\n",
        "model_char = CharLSTM(vocab_size_char, embed_dim=128, hidden_dim=256, num_layers=1).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_char = torch.optim.Adam(model_char.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 10  # you may increase or decrease this : lossê°’ í–¥ìƒì„ ìœ„í•´ 5íšŒì—ì„œ 10íšŒë¡œ ì¡°ì •\n",
        "\n",
        "\n",
        "def train_char_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)   # (B, L)\n",
        "        y = y.to(device)   # (B, L)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # TODO: forward pass, compute loss, backprop, optimizer step\n",
        "\n",
        "\n",
        "        # ====== YOUR CODE HERE ======\n",
        "        # Word Levelê³¼ì™€ ìœ ì‚¬\n",
        "        logits, _ = model(x)\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # ============================\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_char_epoch(model_char, loader_char, optimizer_char, criterion, device)\n",
        "    print(f\"[Char LM] Epoch {epoch}: train loss = {train_loss:.4f}\")"
      ],
      "metadata": {
        "id": "pJYTycALCbxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3705015-bcd9-4ad4-e7fe-c1f4c989232f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Char LM] Epoch 1: train loss = 1.2630\n",
            "[Char LM] Epoch 2: train loss = 1.0894\n",
            "[Char LM] Epoch 3: train loss = 1.0491\n",
            "[Char LM] Epoch 4: train loss = 1.0320\n",
            "[Char LM] Epoch 5: train loss = 1.0219\n",
            "[Char LM] Epoch 6: train loss = 1.0148\n",
            "[Char LM] Epoch 7: train loss = 1.0094\n",
            "[Char LM] Epoch 8: train loss = 1.0054\n",
            "[Char LM] Epoch 9: train loss = 1.0021\n",
            "[Char LM] Epoch 10: train loss = 0.9991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generation (sampling)\n",
        "\n",
        "def generate_characters(model, start_text: str, max_len: int = 300, temperature: float = 1.0) -> str:\n",
        "    \"\"\"\n",
        "    Generate text from a trained character-level language model.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    chars = list(start_text)\n",
        "\n",
        "    # Convert initial prompt to indices\n",
        "    idxs = [char_to_idx.get(c, 0) for c in chars]  # unknown chars -> 0\n",
        "    x = torch.tensor([idxs], dtype=torch.long, device=device)  # (1, L0)\n",
        "\n",
        "    hidden = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # First, run the prompt through the model to update hidden state\n",
        "        if x.size(1) > 0:\n",
        "            logits, hidden = model(x, hidden)\n",
        "\n",
        "        # Now, generate new characters step by step\n",
        "        for _ in range(max_len):\n",
        "            # Use the last character as input\n",
        "            last_idx = idxs[-1]\n",
        "            x_last = torch.tensor([[last_idx]], dtype=torch.long, device=device)  # (1, 1)\n",
        "\n",
        "            logits, hidden = model(x_last, hidden)  # logits: (1, 1, V)\n",
        "            last_logits = logits[:, -1, :] / temperature  # (1, V)\n",
        "            probs = F.softmax(last_logits, dim=-1)        # (1, V)\n",
        "\n",
        "            # Sample next character index\n",
        "            next_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "            idxs.append(next_idx)\n",
        "            chars.append(idx_to_char[next_idx])\n",
        "\n",
        "    return \"\".join(chars)\n"
      ],
      "metadata": {
        "id": "DX15sEt4CtM5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_characters(model_char, start_text=\"KING \", max_len=300, temperature=0.8))\n"
      ],
      "metadata": {
        "id": "3hohroUKCwSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbae27da-b74e-48e8-d755-b1071e95f932"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KING Webtry hows is scarce therefore what a shrewd none of your honour!\n",
            "\n",
            "BALTHASAR:\n",
            "It is, her love I piercing to part out.\n",
            "\n",
            "Provost:\n",
            "I will, my lord.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Dost thou speak with him?\n",
            "\n",
            "Second Citizen:\n",
            "Ere they do preserve our statue of it.\n",
            "\n",
            "PETRUCHIO:\n",
            "Why, then the king from her his.\n",
            "\n",
            "CORIOLANU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_characters(model_char, start_text=\"The Sun \", max_len=300, temperature=0.8))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHVWv0CZAx_E",
        "outputId": "42f698de-95e1-467c-a7db-c4179c0ec19c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Sun her he for your kindness, be utter waged:\n",
            "Therefore they do change their wrongs our marting.\n",
            "\n",
            "Shriefer:\n",
            "It shall be battle have, and am remain as he.\n",
            "\n",
            "ANGELO:\n",
            "What wilt thou be long?\n",
            "\n",
            "CORIOLANUS:\n",
            "Let them hear your honour.\n",
            "\n",
            "Lord:\n",
            "But that he was a golden crokes, new commit you.\n",
            "\n",
            "CORIOLANUS:\n",
            "To the o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_characters(model_char, start_text=\"SCIENCE \", max_len=300, temperature=0.8))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZAdd3VGA1aS",
        "outputId": "b86261ca-74c2-45f1-dd50-a53dad0bb64c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCIENCE YOR OF YORK:\n",
            "Ah, go him, for none can fit the realm\n",
            "My dame is too living farewell of;\n",
            "For Edward mutiness speak not repent their way.\n",
            "Why, that can your datesty and lorn to touch his word:\n",
            "Lies, and so sees thy adversague,\n",
            "As I hear, say we on my pedant.\n",
            "\n",
            "BRUTUS:\n",
            "Good night against my name is too m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Character ë‹¨ìœ„ë¡œ ìƒì„±ì„ í•  ê²½ìš° ë‹¨ì–´ ë‹¨ìœ„ ìƒì„±ë³´ë‹¤ ì„±ëŠ¥ì´ë‚˜ ì •í™•ë„ê°€ í¬ê²Œ ë–¨ì–´ì§€ëŠ” ëª¨ìŠµì„ ë³´ì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "akva-NJwa8iN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "í•˜ì§€ë§Œ ì¼ë¶€ ë‹¨ì–´ì— ëŒ€í•´ì„œëŠ” ê³ ì–´ì²´ì— ëŒ€í•œ ìƒì„± ë° í¬ê³¡ í˜•ì‹ì˜ í˜•íƒœì  êµ¬ì„±ì„ í•™ìŠµ ë°ì´í„°ì™€ ìµœëŒ€í•œ ìœ ì‚¬í•˜ê²Œ ë§ì¶”ë ¤ëŠ” ë…¸ë ¥ì´ ë³´ì˜€ìŠµë‹ˆë‹¤.\n",
        "íŠ¹íˆ ì…ë ¥ ë°ì´í„°ê°€ Capitalì¸ ê²½ìš°(SCIENCE) ë“±ì¥ì¸ë¬¼ë¡œ ì¸ì‹í•˜ì—¬ êµ¬ì„±í•˜ëŠ” ì ì´ í¥ë¯¸ë¡œì› ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "N2PsCptgbF6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM ëª¨ë¸ì˜ ê²½ìš°ì¥ê¸°ê¸°ì–µì„ ê¸°ë°˜ìœ¼ë¡œ ë” ë‚˜ì€ ìƒì„±ì„ ì§„í–‰í•  ìˆ˜ ìˆëŠ” ê²ƒì´ ì´ë¡ ì´ì§€ë§Œ, ì´ë²ˆ ì—°êµ¬ì—ì„œëŠ” ê°œì„ ì ì„ ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n",
        "í–¥í›„ í›„ì† ì—°êµ¬ì—ì„œ LSTM ê¸°ë°˜ì˜ Word-Level Generation Modelì„ ì‹œë„í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "17fzFmgQbk7p"
      }
    }
  ]
}